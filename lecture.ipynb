{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea26d20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# Машинне навчання для промислової робототехніки\n",
    "## Навчання та впровадження моделей комп'ютерного зору для автоматизації\n",
    "\n",
    "---\n",
    "\n",
    "![Промислове компютерне бачення](assets/Vision%20Tech.jpg)\n",
    "\n",
    "*Фото: Промисловий робот з комп'ютерним зором*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c29110",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## План Заняття\n",
    "\n",
    "Сьогодні ми розглянемо повний робочий процес впровадження машинного зору в промисловій робототехніці:\n",
    "\n",
    "1. **Вступ до МН в промисловій робототехніці**\n",
    "   - Переваги та ключові застосування\n",
    "   - Як МН трансформує традиційні підходи\n",
    "\n",
    "2. **Збір даних та маркування з Roboflow**\n",
    "   - Налаштування проектів\n",
    "   - Створення якісних навчальних наборів даних\n",
    "\n",
    "3. **Завантаження та підготовка наборів даних**\n",
    "   - Структура набору даних для навчання\n",
    "   - Налаштування конфігураційного файлу\n",
    "\n",
    "4. **Локальне навчання моделі комп'ютерного зору**\n",
    "   - Використання YOLOv8 для виявлення об'єктів\n",
    "   - Оптимізація для різного апаратного забезпечення\n",
    "\n",
    "5. **Розгортання для виведення з USB-камерою**\n",
    "   - Виявлення об'єктів у реальному часі\n",
    "   - Аспекти продуктивності\n",
    "\n",
    "6. **Демонстрація та питання-відповіді**\n",
    "   - Жива демонстрація\n",
    "   - Відкрита дискусія"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd3435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Вступ до МН в промисловій робототехніці\n",
    "\n",
    "Машинне навчання революціонізувало промислову робототехніку, дозволяючи системам \"бачити\" та інтелектуально взаємодіяти з навколишнім середовищем. Розглянемо ключові аспекти:\n",
    "\n",
    "- **Переваги МН в робототехніці**\n",
    "- **Ключові застосування** \n",
    "- **Традиційні підходи у порівнянні з підходами на основі МН**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06021e30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Переваги МН в робототехніці\n",
    "\n",
    "- **Підвищення ефективності**: Скорочення циклу виробництва та збільшення продуктивності\n",
    "  \n",
    "- **Покращена точність**: Підвищення точності виявлення та маніпулювання об'єктами\n",
    "  \n",
    "- **Більша гнучкість**: Адаптація до нових продуктів без перепрограмування\n",
    "  \n",
    "- **Зниження витрат**: Зменшення витрат на робочу силу та відходів через помилки\n",
    "  \n",
    "- **Покращення безпеки**: Краще виявлення людей у робочому просторі"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2717e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Ключові застосування\n",
    "\n",
    "1. **Ідентифікація об'єктів**:\n",
    "   - Розпізнавання різноманітних деталей на виробничій лінії\n",
    "   - Визначення положення об'єктів у 3D-просторі\n",
    "   - Класифікація типів продукції\n",
    "\n",
    "2. **Відстеження об'єктів**:\n",
    "   - Моніторинг руху деталей на виробничій лінії\n",
    "   - Одночасне відстеження декількох об'єктів\n",
    "   - Забезпечення координації між різними роботами\n",
    "\n",
    "3. **Захоплення та розміщення**:\n",
    "   - Визначення оптимальних точок захоплення\n",
    "   - Робота з об'єктами різних форм і розмірів\n",
    "   - Адаптація до змінних умов середовища"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afc078",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Традиційні підходи vs МН-підходи\n",
    "\n",
    "<div style=\"font-size: 120%;\">\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th><strong>Аспект</strong></th>\n",
    "<th><strong>Традиційний підхід</strong></th>\n",
    "<th><strong>Підхід на основі МН</strong></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><strong>Програмування</strong></td>\n",
    "<td>Жорстко запрограмовані правила</td>\n",
    "<td>Навчання на прикладах</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>Адаптивність</strong></td>\n",
    "<td>Обмежена, потребує перепрограмування</td>\n",
    "<td>Висока, адаптується до змін</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>Точність</strong></td>\n",
    "<td>Добра в структурованому середовищі</td>\n",
    "<td>Висока навіть у складних умовах</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>Розробка</strong></td>\n",
    "<td>Тривала і трудомістка</td>\n",
    "<td>Швидша з існуючими моделями</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>Обробка винятків</strong></td>\n",
    "<td>Потребує окремих правил для кожного випадку</td>\n",
    "<td>Узагальнює на основі досвіду</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5b59a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Збір даних та маркування з Roboflow\n",
    "\n",
    "Перший крок у створенні моделі машинного зору — це збір та маркування навчальних даних. \n",
    "\n",
    "Roboflow — це платформа, яка спрощує цей процес, особливо для початківців.\n",
    "\n",
    "![Roboflow інтерфейс маркування](assets/annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2ef95",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Демонстрація Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd903df0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Створення якісних навчальних наборів даних\n",
    "\n",
    "### Збір зображень:\n",
    "\n",
    "- **Методи збору**:\n",
    "  - Знімки з камери робота в реальному середовищі\n",
    "  - Запис відео та вилучення кадрів\n",
    "  - Використання існуючих наборів даних з доповненнями\n",
    "\n",
    "- **Рекомендації для якісних даних**:\n",
    "  - Включіть зображення з різних кутів та відстаней\n",
    "  - Зафіксуйте варіації освітлення\n",
    "  - Включіть різні фони та можливі перешкоди\n",
    "  - Мінімум 50 зображень на клас для простого доказу концепції\n",
    "\n",
    "### Підготовка даних:\n",
    "\n",
    "1. **Маркування зображень**:\n",
    "   - Окреслення обмежувальних рамок навколо об'єктів\n",
    "   - Присвоєння класу (мітки) кожній рамці\n",
    "   - Перегляд та коригування міток за потреби\n",
    "\n",
    "2. **Обробка даних**:\n",
    "   - **Розділення набору даних**: Автоматичний поділ на тренувальну, валідаційну та тестову вибірки\n",
    "   - **Аугментація**: Створення додаткових тренувальних зразків через обертання, віддзеркалення, регулювання яскравості тощо\n",
    "   - **Попередня обробка**: Зміна розміру зображень, коригування кольорів тощо"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12ed93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Завантаження та підготовка наборів даних\n",
    "\n",
    "Після маркування даних у Roboflow, необхідно завантажити їх для локального навчання моделі. Розглянемо як це зробити та підготувати дані для тренування.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c051a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Структура набору даних для навчання\n",
    "\n",
    "1. **Структура каталогів YOLO**:\n",
    "   - `/data/назва_проекту/` - основний каталог\n",
    "   - `/train/` - зображення та мітки для навчання\n",
    "   - `/valid/` - зображення та мітки для валідації\n",
    "   - `/test/` - зображення та мітки для тестування\n",
    "\n",
    "2. **Формат міток**:\n",
    "   - Текстові файли `.txt` для кожного зображення\n",
    "   - Кожен рядок містить: `клас x_центр y_центр ширина висота`\n",
    "   - Координати нормалізовані від 0 до 1\n",
    "\n",
    "3. **Організація даних**:\n",
    "   - Зображення: JPG або PNG формати\n",
    "   - Однакове розміщення файлів зображень та міток\n",
    "   - Належне розділення між навчальним, валідаційним та тестовим наборами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc505bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Налаштування конфігураційного файлу\n",
    "\n",
    "1. **Файл data.yaml**:\n",
    "   ```yaml\n",
    "   path: ../data/назва_проекту  # Шлях до набору даних\n",
    "   train: train/images          # Зображення для тренування\n",
    "   val: valid/images            # Зображення для валідації\n",
    "   test: test/images            # Зображення для тестування\n",
    "   \n",
    "   nc: 2                        # Кількість класів\n",
    "   names: ['об'єкт1', 'об'єкт2'] # Назви класів"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ea8ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Завантаження через Roboflow API\n",
    "\n",
    "Після маркування даних у Roboflow, необхідно завантажити їх для локального навчання моделі. Розглянемо як це зробити та підготувати дані для тренування."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a306014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.20.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.3.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.0.3)\n",
      "Requirement already satisfied: matplotlib>=3.4.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (3.7.5)\n",
      "Requirement already satisfied: jupyter>=1.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: ipykernel>=6.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (6.29.5)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: opencv-python>=4.5.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (4.11.0.86)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.19.1)\n",
      "Requirement already satisfied: roboflow>=0.2.29 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (1.1.64)\n",
      "Requirement already satisfied: ultralytics>=8.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (8.3.140)\n",
      "Requirement already satisfied: tqdm>=4.62.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
      "Requirement already satisfied: Pillow>=9.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (10.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.0->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.8/site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 4)) (6.4.5)\n",
      "Requirement already satisfied: notebook in ./venv/lib/python3.8/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (7.3.3)\n",
      "Requirement already satisfied: jupyter-console in ./venv/lib/python3.8/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in ./venv/lib/python3.8/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (7.16.6)\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.8/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: jupyterlab in ./venv/lib/python3.8/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (4.3.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (8.12.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./venv/lib/python3.8/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 6)) (5.14.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (4.13.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./venv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->-r requirements.txt (line 11)) (12.9.41)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (2025.4.26)\n",
      "Requirement already satisfied: idna==3.7 in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (3.7)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (4.10.0.84)\n",
      "Requirement already satisfied: pillow-heif>=0.18.0 in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (0.18.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (2.32.3)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (2.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (1.0.0)\n",
      "Requirement already satisfied: filetype in ./venv/lib/python3.8/site-packages (from roboflow>=0.2.29->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./venv/lib/python3.8/site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 14)) (1.10.1)\n",
      "Requirement already satisfied: py-cpuinfo in ./venv/lib/python3.8/site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 14)) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in ./venv/lib/python3.8/site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 14)) (2.0.14)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.4.0->-r requirements.txt (line 4)) (3.20.2)\n",
      "Requirement already satisfied: backcall in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.19.2)\n",
      "Requirement already satisfied: pickleshare in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (2.19.1)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (4.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in ./venv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=6.0.0->-r requirements.txt (line 6)) (8.5.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.0.0->-r requirements.txt (line 6)) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests->roboflow>=0.2.29->-r requirements.txt (line 13)) (3.4.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./venv/lib/python3.8/site-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 5)) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./venv/lib/python3.8/site-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.0.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 11)) (2.1.5)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in ./venv/lib/python3.8/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in ./venv/lib/python3.8/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.28.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./venv/lib/python3.8/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./venv/lib/python3.8/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.14.2)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in ./venv/lib/python3.8/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in ./venv/lib/python3.8/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in ./venv/lib/python3.8/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (44.0.0)\n",
      "Requirement already satisfied: tomli>=1.2.2 in ./venv/lib/python3.8/site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in ./venv/lib/python3.8/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.10.1)\n",
      "Requirement already satisfied: nbformat>=5.7 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./venv/lib/python3.8/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.8/site-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: webencodings in ./venv/lib/python3.8/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.3,>=1.1.0 in ./venv/lib/python3.8/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.8/site-packages (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (4.5.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.8/site-packages (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.8.4)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in ./venv/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in ./venv/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in ./venv/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in ./venv/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in ./venv/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.21.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in ./venv/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./venv/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in ./venv/lib/python3.8/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in ./venv/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in ./venv/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in ./venv/lib/python3.8/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (4.23.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./venv/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.21.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.8/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.7)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 6)) (0.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.8/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.8/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./venv/lib/python3.8/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (21.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./venv/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in ./venv/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.8/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.20.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./venv/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: rfc3339-validator in ./venv/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./venv/lib/python3.8/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in ./venv/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in ./venv/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./venv/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: uri-template in ./venv/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in ./venv/lib/python3.8/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (24.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./venv/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./venv/lib/python3.8/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in ./venv/lib/python3.8/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.9.0.20241206)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Встановимо або перевіримо наявність бібліотек, які потрібні для роботи\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d26432b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Версія PyTorch: 2.4.1+cu121\n",
      "Використовується CUDA\n"
     ]
    }
   ],
   "source": [
    "# Імпортуємо необхідні бібліотеки\n",
    "from roboflow import Roboflow\n",
    "import torch\n",
    "\n",
    "# Перевірка доступності CUDA (не застосовується для Apple Silicon, але корисно включити для портативності)\n",
    "print(f\"Версія PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Використовується CUDA\")\n",
    "# Перевірка доступності MPS (Metal Performance Shaders) для Apple Silicon\n",
    "elif hasattr(torch.backends, 'mps') and hasattr(torch.backends.mps, 'is_available'):\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Використовується MPS (Metal Performance Shaders)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS не доступний у цій версії PyTorch, використовується CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aaaa02",
   "metadata": {},
   "source": [
    "### Завантаження даних з Roboflow\n",
    "\n",
    "Щоб завантажити дані з Roboflow, вам знадобиться API-ключ та інформація про проект:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42cc1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in data/robotics_dataset_v3 to yolov8:: 100%|██████████| 346819/346819 [00:12<00:00, 26905.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to data/robotics_dataset_v3 in yolov8:: 100%|██████████| 3236/3236 [00:00<00:00, 4351.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завантаження завершено.\n",
      "Набір даних успішно завантажено до: /home/levko/repos/kai-lecture/data/robotics_dataset_v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Завантаження даних з Roboflow\n",
    "# Зверніть увагу, що для використання Roboflow API вам потрібно зареєструватися на їхньому сайті та отримати API ключ\n",
    "# Зазначте, що ви повинні мати файл .env з вашим API ключем\n",
    "# або ви можете вказати його безпосередньо в коді (не рекомендується з міркувань безпеки)\n",
    "\n",
    "# Також, необхідно вказати назву вашого проекту та версію набору даних\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Завантаження змінних середовища з файлу .env\n",
    "# Переконайтеся, що у вас є файл .env з вашим API ключем\n",
    "load_dotenv()\n",
    "\n",
    "# Отримання API ключа з змінних середовища\n",
    "api_key = os.getenv('ROBOFLOW_API_KEY')\n",
    "\n",
    "# Визначення версії набору даних\n",
    "dataset_version = 3  # Змініть на вашу фактичну версію набору даних\n",
    "\n",
    "rf = Roboflow(api_key=api_key)\n",
    "project = rf.workspace().project(\"orange-ball-detection\")\n",
    "version = project.version(dataset_version)  # Змініть 1 на ваш фактичний номер версії\n",
    "\n",
    "# Завантаження набору даних у форматі YOLO (сумісний з YOLOv8)\n",
    "dataset = version.download(model_format=\"yolov8\", location=f\"data/robotics_dataset_v{dataset_version}\", overwrite=False)\n",
    "\n",
    "print(\"Завантаження завершено.\")\n",
    "print(\"Набір даних успішно завантажено до:\", dataset.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc8500",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Локальне навчання моделі комп'ютерного зору\n",
    "\n",
    "Тепер, коли ми підготували наші дані, настав час навчити модель комп'ютерного зору. Ми будемо використовувати базову модель YOLOv8n (YOLOv8 Nano) для виявлення об'єктів. Це легка модель, яка підходить для навчання на комп'ютерах з обмеженими ресурсами, таких як MacBook з M1.\n",
    "\n",
    "### Варіанти навчання\n",
    "\n",
    "1. **Локальне навчання на Mac M1**:\n",
    "   - Підходить для невеликих наборів даних та швидких демонстрацій\n",
    "   - Використовує Metal Performance Shaders (MPS) для прискорення\n",
    "   - Обмежено доступною пам'яттю та обчислювальною потужністю\n",
    "2. **Навчання на (віддаленому) високопродуктивному GPU (рекомендується)**:\n",
    "   - Значно швидше навчання з GPU, такими як NVIDIA 3090Ti\n",
    "   - Може обробляти більші моделі (середні, великі) та більше даних\n",
    "   - Створіть таку ж структуру каталогів на віддаленій машині\n",
    "   - Скопіюйте свій набір даних на віддалену машину\n",
    "   - Запустіть код навчання там і завантажте файл ваги після завершення"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97b66cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.141 available 😃 Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=ram, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/home/levko/repos/kai-lecture/data/robotics_dataset_v3/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=robotics_model_v32, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=models, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=models/robotics_model_v32, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=16, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:00<00:00, 13.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 6370.0±1332.8 MB/s, size: 217.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/levko/repos/kai-lecture/data/robotics_dataset_v3/train/labels.cache... 1043 images, 161 backgrounds, 0 corrupt: 100%|██████████| 1043/1043 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB RAM): 100%|██████████| 1043/1043 [00:00<00:00, 2238.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1522.4±840.6 MB/s, size: 228.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/levko/repos/kai-lecture/data/robotics_dataset_v3/valid/labels.cache... 346 images, 46 backgrounds, 0 corrupt: 100%|██████████| 346/346 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100%|██████████| 346/346 [00:02<00:00, 166.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to models/robotics_model_v32/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 16 dataloader workers\n",
      "Logging results to \u001b[1mmodels/robotics_model_v32\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]Exception in thread Thread-386:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/levko/repos/kai-lecture/venv/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/levko/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/levko/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 32, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/levko/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 496, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "  0%|          | 0/17 [00:01<?, ?it/s]\n",
      "Exception in thread Thread-398:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/levko/repos/kai-lecture/venv/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/levko/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/levko/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 32, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/levko/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 496, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Тренування моделі\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Зверніть увагу, що для реального навчання вам слід використовувати більше епох\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# та налаштувати параметри навчання відповідно до ваших потреб\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Для демонстраційних цілей ми зменшили кількість епох до 10\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_yaml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Шлях до файлу data.yaml\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Кількість епох\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Кількість епох без покращення для ранньої зупинки\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Розмір зображення (640px)\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Зберігати модель кожні 5 епох для економії дискового простору\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Пристрій для навчання (визначили вище)\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Збільшений розмір пакету для 3090Ti\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Збільшена кількість потоків для завантаження даних\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Кешувати зображення в RAM для максимальної швидкості (з 64GB RAM)\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Назва проекту для збереження результатів\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrobotics_model_v\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_version\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Назва моделі\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Увімкнути змішану точність для прискорення та економії пам'яті\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcos_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Косинусне затухання швидкості навчання для кращої збіжності\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdamW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Використання AdamW оптимізатора замість SGD для кращої продуктивності\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclose_mosaic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Вимкнути мозаїчну аугментацію на останніх 10 епохах для стабільності\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mНавчання завершено! Модель збережено в ./models/robotics_model_v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/engine/model.py:793\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/engine/trainer.py:211\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/engine/trainer.py:390\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    389\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 390\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/nn/tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/nn/tasks.py:314\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    313\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/utils/loss.py:241\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    237\u001b[0m pred_bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_decode(anchor_points, pred_distri)  \u001b[38;5;66;03m# xyxy, (b, h*w, 4)\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m _, target_bboxes, target_scores, fg_mask, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massigner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\u001b[39;49;00m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m target_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(target_scores\u001b[38;5;241m.\u001b[39msum(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Cls loss\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/utils/tal.py:77\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.forward\u001b[0;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     69\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfull_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbg_idx),\n\u001b[1;32m     70\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_bboxes),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mOutOfMemoryError:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Move tensors to CPU, compute, then move back to original device\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/utils/tal.py:104\u001b[0m, in \u001b[0;36mTaskAlignedAssigner._forward\u001b[0;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Compute the task-aligned assignment.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m        target_gt_idx (torch.Tensor): Target ground truth indices with shape (bs, num_total_anchors).\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     mask_pos, align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pos_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     target_gt_idx, fg_mask, mask_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_highest_overlaps(mask_pos, overlaps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_max_boxes)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Assigned target\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/utils/tal.py:139\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.get_pos_mask\u001b[0;34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_pos_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    Get positive mask for each ground truth box.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m        overlaps (torch.Tensor): Overlaps between predicted and ground truth boxes with shape (bs, max_num_obj, h*w).\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     mask_in_gts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_candidates_in_gts\u001b[49m\u001b[43m(\u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Get anchor_align metric, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts \u001b[38;5;241m*\u001b[39m mask_gt)\n",
      "File \u001b[0;32m~/repos/kai-lecture/venv/lib/python3.8/site-packages/ultralytics/utils/tal.py:296\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.select_candidates_in_gts\u001b[0;34m(xy_centers, gt_bboxes, eps)\u001b[0m\n\u001b[1;32m    294\u001b[0m lt, rb \u001b[38;5;241m=\u001b[39m gt_bboxes\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# left-top, right-bottom\u001b[39;00m\n\u001b[1;32m    295\u001b[0m bbox_deltas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((xy_centers[\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m-\u001b[39m lt, rb \u001b[38;5;241m-\u001b[39m xy_centers[\u001b[38;5;28;01mNone\u001b[39;00m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mview(bs, n_boxes, n_anchors, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbbox_deltas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgt_(eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Імпортуємо необхідні бібліотеки\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Налаштування шляху до файлу data.yaml\n",
    "# Зазначте, що YOLOv8 автоматично генерує data.yaml файл під час завантаження набору даних\n",
    "data_yaml = f\"{dataset.location}/data.yaml\"\n",
    "\n",
    "# Для локального навчання на Mac M1 (для демонстраційних цілей)\n",
    "# Ініціалізуємо попередньо навчену модель YOLOv8n (маленький розмір для швидшого навчання)\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Тренування моделі\n",
    "# Зверніть увагу, що для реального навчання вам слід використовувати більше епох\n",
    "# та налаштувати параметри навчання відповідно до ваших потреб\n",
    "# Для демонстраційних цілей ми зменшили кількість епох до 10\n",
    "results = model.train(\n",
    "    data=data_yaml,     # Шлях до файлу data.yaml\n",
    "    epochs=100,         # Кількість епох\n",
    "    patience=10,        # Кількість епох без покращення для ранньої зупинки\n",
    "    imgsz=640,          # Розмір зображення (640px)\n",
    "    save_period=5,      # Зберігати модель кожні 5 епох для економії дискового простору\n",
    "    device=device,      # Пристрій для навчання (визначили вище)\n",
    "    batch=64,           # Збільшений розмір пакету для 3090Ti\n",
    "    workers=16,         # Збільшена кількість потоків для завантаження даних\n",
    "    cache='ram',        # Кешувати зображення в RAM для максимальної швидкості (з 64GB RAM)\n",
    "    project=\"models\",   # Назва проекту для збереження результатів\n",
    "    name=f\"robotics_model_v{dataset_version}\",  # Назва моделі\n",
    "    amp=True,           # Увімкнути змішану точність для прискорення та економії пам'яті\n",
    "    cos_lr=True,        # Косинусне затухання швидкості навчання для кращої збіжності\n",
    "    optimizer=\"AdamW\",  # Використання AdamW оптимізатора замість SGD для кращої продуктивності\n",
    "    close_mosaic=10     # Вимкнути мозаїчну аугментацію на останніх 10 епохах для стабільності\n",
    ")\n",
    "\n",
    "print(f\"Навчання завершено! Модель збережено в ./models/robotics_model_v{dataset_version}/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c7a76",
   "metadata": {},
   "source": [
    "### Поради та оптимізація навчання\n",
    "\n",
    "#### Для Apple Silicon\n",
    "\n",
    "1. **Використовуйте прискорення MPS**: PyTorch 2.0+ підтримує Metal Performance Shaders (MPS) для Mac M1/M2.\n",
    "\n",
    "2. **Оптимізація розміру пакету**: Почніть з розміру пакету 16 та коригуйте відповідно до доступної пам'яті.\n",
    "\n",
    "3. **Вибір розміру моделі**: Використовуйте менші моделі, такі як YOLOv8n або YOLOv8s, для швидшого локального навчання.\n",
    "\n",
    "4. **Кешування даних**: Використовуйте параметр `cache=True` для прискорення навчання.\n",
    "\n",
    "#### Для NVIDIA 3090Ti або інших GPU\n",
    "\n",
    "1. **Більший розмір пакету**: Використовуйте розміри пакетів 32 або 64, щоб повністю використовувати пам'ять GPU.\n",
    "\n",
    "2. **Кілька робочих потоків**: Збільште значення `workers` до 8 або 16 для швидшого завантаження даних.\n",
    "\n",
    "3. **Більші моделі**: Використовуйте переваги потужного GPU, застосовуючи моделі YOLOv8m або YOLOv8l.\n",
    "\n",
    "4. **Змішана точність**: Додайте `amp=True` для використання навчання зі змішаною точністю для кращої продуктивності.\n",
    "\n",
    "5. **Більше епох**: Навчайте модель протягом 50-100 епох для кращої конвергенції.\n",
    "\n",
    "#### Загальні поради\n",
    "\n",
    "1. **Трансферне навчання**: Ми використовуємо попередньо навчені моделі та дооптимізуємо їх, що значно пришвидшує навчання.\n",
    "\n",
    "2. **Аугментація зображень**: Використовуйте можливості аугментації Roboflow для збільшення різноманітності набору даних."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02015e",
   "metadata": {},
   "source": [
    "## Deploying the Model for Inference with a USB Camera\n",
    "## Запуск моделі з USB-камерою\n",
    "\n",
    "З натренованою моделлю ми можемо використовувати її для виявлення об'єктів у реальному часі за допомогою USB-камери."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df098668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпортуємо необхідні бібліотеки\n",
    "# Примітка: цей код призначений для запуску в окремому Python скрипті, а не в Jupyter\n",
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import argparse\n",
    "\n",
    "# Зберемо шлях до моделі\n",
    "model_path = f\"./models/robotics_model_v{dataset_version}/weights/best.pt\"\n",
    "\n",
    "# Функція для запуску об'єктного виявлення на USB-камері\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Запуск об\\'єктного виявлення на USB-камері')\n",
    "    parser.add_argument('--model', type=str, default='yolov8n.pt', \n",
    "                        help='Шлях до моделі YOLOv8')\n",
    "    parser.add_argument('--conf', type=float, default=0.25, \n",
    "                        help='Поріг впевненості для виявлення об\\'єктів')\n",
    "    parser.add_argument('--camera', type=int, default=0, \n",
    "                        help='Індекс камери (зазвичай 0 для вбудованої, 1 для USB)')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Завантаження моделі\n",
    "    try:\n",
    "        model = YOLO(args.model)\n",
    "        print(f\"Завантажено модель з {args.model}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Помилка при завантаженні моделі: {e}\")\n",
    "        return\n",
    "\n",
    "    # Відкриття камери\n",
    "    cap = cv2.VideoCapture(args.camera)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Помилка: Не вдалося відкрити камеру з індексом {args.camera}\")\n",
    "        print(\"Спробуйте інший індекс камери, використовуючи аргумент --camera\")\n",
    "        return\n",
    "\n",
    "    # Налаштування параметрів камери\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    print(\"Камера успішно відкрита. Натисніть 'q', щоб вийти.\")\n",
    "\n",
    "    # Змінні для обчислення FPS\n",
    "    fps = 0\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        # Зчитування кадру з камери\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Помилка: Не вдалося зчитати зображення з камери\")\n",
    "            break\n",
    "\n",
    "        # Збільшення лічильника кадрів  \n",
    "        frame_count += 1\n",
    "\n",
    "        # Виконання інференсу\n",
    "        results = model(frame, conf=args.conf)\n",
    "\n",
    "        # Обробка результатів\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Обчислення FPS\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time >= 1.0:\n",
    "            fps = frame_count / elapsed_time\n",
    "            frame_count = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "        cv2.putText(annotated_frame, f\"FPS: {fps:.2f}\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Відображення кадру\n",
    "        cv2.imshow('Об\\'єктне виявлення', annotated_frame)\n",
    "        \n",
    "        # Вихід з циклу при натисканні 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Завершення роботи\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a906db",
   "metadata": {},
   "source": [
    "## Запуск скрипта для виведення\n",
    "\n",
    "Щоб запустити скрипт виведення з вашою USB-камерою:\n",
    "\n",
    "1. Переконайтеся, що ваше віртуальне середовище активоване:\n",
    "   ```\n",
    "   source venv/bin/activate  # На macOS/Linux\n",
    "   ```\n",
    "   або\n",
    "   ```\n",
    "   venv\\Scripts\\activate  # На Windows\n",
    "   ```\n",
    "2. Запустіть скрипт з терміналу:\n",
    "   ```\n",
    "   python run_robot_vision.py\n",
    "   ```\n",
    "3. Ви можете налаштувати поведінку за допомогою цих параметрів:\n",
    "   - Щоб використовувати певну камеру: `--camera 1` \n",
    "   - Щоб використовувати вашу натреновану модель: `--model ./models/robotics_model/weights/best.pt`\n",
    "   - Щоб налаштувати впевненість виявлення: `--conf 0.4`\n",
    "\n",
    "Для прикладу:\n",
    "\n",
    "```\n",
    "python run_robot_vision.py --camera 1 --model ./models/robotics_model/weights/best.pt --conf 0.4\n",
    "```\n",
    "\n",
    "Якщо у вас виникають проблеми з індексом камери, спробуйте різні значення (0, 1, 2 тощо), поки не знайдете те, що відповідає вашій USB-камері."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bded456",
   "metadata": {},
   "source": [
    "## Висновок та наступні кроки\n",
    "\n",
    "Вітаємо! Тепер ви успішно:\n",
    "\n",
    "1. ✅ Дізналися, як машинне навчання може покращити завдання промислової робототехніки  \n",
    "2. ✅ Створили й проанотували набір даних за допомогою Roboflow  \n",
    "3. ✅ Підготували дані для локального навчання  \n",
    "4. ✅ Навчили власну модель комп’ютерного зору, оптимізовану для вашого M1 Mac  \n",
    "5. ✅ Розгорнули модель для реального часу з USB-камерою  \n",
    "\n",
    "### Що робити далі\n",
    "\n",
    "Тепер, коли у вас є базові навички, ось кілька шляхів для подальшого розвитку:\n",
    "\n",
    "1. **Створіть більший набір даних**: спробуйте зібрати більше знімків у різних умовах освітлення та під різними кутами, щоб підвищити стійкість моделі.  \n",
    "2. **Експериментуйте з різними моделями**: YOLOv8 доступний у різних розмірах (nano, small, medium, large). Навчіть той самий набір даних на різних архітектурах і порівняйте швидкість та точність. Також спробуйте інші моделі YOLO (наприклад, YOLOv11). \n",
    "3. **Оптимізація швидкодії**: досліджуйте методи квантизації або обрізки (pruning), щоб модель працювала ще швидше на низько ресурсних пристроях які будуть встановлені безпосередньо біля роботів.  \n",
    "4. **Налаштування мультикамерної системи**: для більш складних застосунків встановіть кілька камер для отримання різних ракурсів тієї самої сцени.  \n",
    "\n",
    "### Корисні ресурси\n",
    "\n",
    "- [Документація Ultralytics YOLOv8](https://docs.ultralytics.com/) — детальні поради щодо роботи з моделлю  \n",
    "- [Документація Roboflow](https://docs.roboflow.com/) — розширені прийоми аугментації та розмітки даних  \n",
    "- [Документація PyTorch](https://pytorch.org/docs/) — щоб зрозуміти фреймворк глибинного навчання  \n",
    "- [OpenCV Python Tutorials](https://docs.opencv.org/master/d6/d00/tutorial_py_root.html) — для просунутих операцій комп’ютерного зору  \n",
    "\n",
    "Не забувайте тримати віртуальне середовище активним під час роботи над проєктом та не соромтеся експериментувати з моделлю на різних об’єктах!  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "height": "100%",
   "scroll": false,
   "transition": "fade",
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
