{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea26d20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# Машинне навчання для промислової робототехніки\n",
    "## Навчання та впровадження моделей комп'ютерного зору для автоматизації\n",
    "\n",
    "---\n",
    "\n",
    "![Промислове компютерне бачення](assets/Vision%20Tech.jpg)\n",
    "\n",
    "*Фото: Промисловий робот з комп'ютерним зором*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c29110",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## План Заняття\n",
    "\n",
    "Сьогодні ми розглянемо повний робочий процес впровадження машинного зору в промисловій робототехніці:\n",
    "\n",
    "1. **Вступ до МН в промисловій робототехніці**\n",
    "   - Переваги та ключові застосування\n",
    "   - Як МН трансформує традиційні підходи\n",
    "\n",
    "2. **Збір даних та маркування з Roboflow**\n",
    "   - Налаштування проектів\n",
    "   - Створення якісних навчальних наборів даних\n",
    "\n",
    "3. **Завантаження та підготовка наборів даних**\n",
    "   - Структура набору даних для навчання\n",
    "   - Налаштування конфігураційного файлу\n",
    "\n",
    "4. **Локальне навчання моделі комп'ютерного зору**\n",
    "   - Використання YOLOv8 для виявлення об'єктів\n",
    "   - Оптимізація для різного апаратного забезпечення\n",
    "\n",
    "5. **Розгортання для виведення з USB-камерою**\n",
    "   - Виявлення об'єктів у реальному часі\n",
    "   - Аспекти продуктивності\n",
    "\n",
    "6. **Демонстрація та питання-відповіді**\n",
    "   - Жива демонстрація\n",
    "   - Відкрита дискусія"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd3435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Вступ до МН в промисловій робототехніці\n",
    "\n",
    "Машинне навчання революціонізувало промислову робототехніку, дозволяючи системам \"бачити\" та інтелектуально взаємодіяти з навколишнім середовищем. Розглянемо ключові аспекти:\n",
    "\n",
    "- **Переваги МН в робототехніці**\n",
    "- **Ключові застосування** \n",
    "- **Традиційні підходи у порівнянні з підходами на основі МН**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06021e30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Переваги МН в робототехніці\n",
    "\n",
    "- **Підвищення ефективності**: Скорочення циклу виробництва та збільшення продуктивності\n",
    "  \n",
    "- **Покращена точність**: Підвищення точності виявлення та маніпулювання об'єктами\n",
    "  \n",
    "- **Більша гнучкість**: Адаптація до нових продуктів без перепрограмування\n",
    "  \n",
    "- **Зниження витрат**: Зменшення витрат на робочу силу та відходів через помилки\n",
    "  \n",
    "- **Покращення безпеки**: Краще виявлення людей у робочому просторі"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2717e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Ключові застосування\n",
    "\n",
    "1. **Ідентифікація об'єктів**:\n",
    "   - Розпізнавання різноманітних деталей на виробничій лінії\n",
    "   - Визначення положення об'єктів у 3D-просторі\n",
    "   - Класифікація типів продукції\n",
    "\n",
    "2. **Відстеження об'єктів**:\n",
    "   - Моніторинг руху деталей на виробничій лінії\n",
    "   - Одночасне відстеження декількох об'єктів\n",
    "   - Забезпечення координації між різними роботами\n",
    "\n",
    "3. **Захоплення та розміщення**:\n",
    "   - Визначення оптимальних точок захоплення\n",
    "   - Робота з об'єктами різних форм і розмірів\n",
    "   - Адаптація до змінних умов середовища"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afc078",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Традиційні підходи vs МН-підходи\n",
    "\n",
    "<div style=\"font-size: 120%;\">\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th><strong>Аспект</strong></th>\n",
    "<th><strong>Традиційний підхід</strong></th>\n",
    "<th><strong>Підхід на основі МН</strong></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><strong>Програмування</strong></td>\n",
    "<td>Жорстко запрограмовані правила</td>\n",
    "<td>Навчання на прикладах</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>Адаптивність</strong></td>\n",
    "<td>Обмежена, потребує перепрограмування</td>\n",
    "<td>Висока, адаптується до змін</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>Точність</strong></td>\n",
    "<td>Добра в структурованому середовищі</td>\n",
    "<td>Висока навіть у складних умовах</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>Розробка</strong></td>\n",
    "<td>Тривала і трудомістка</td>\n",
    "<td>Швидша з існуючими моделями</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>Обробка винятків</strong></td>\n",
    "<td>Потребує окремих правил для кожного випадку</td>\n",
    "<td>Узагальнює на основі досвіду</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5b59a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Збір даних та маркування з Roboflow\n",
    "\n",
    "Перший крок у створенні моделі машинного зору — це збір та маркування навчальних даних. \n",
    "\n",
    "Roboflow — це платформа, яка спрощує цей процес, особливо для початківців.\n",
    "\n",
    "![Roboflow інтерфейс маркування](assets/annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2ef95",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Демонстрація Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd903df0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Створення якісних навчальних наборів даних\n",
    "\n",
    "### Збір зображень:\n",
    "\n",
    "- **Методи збору**:\n",
    "  - Знімки з камери робота в реальному середовищі\n",
    "  - Запис відео та вилучення кадрів\n",
    "  - Використання існуючих наборів даних з доповненнями\n",
    "\n",
    "- **Рекомендації для якісних даних**:\n",
    "  - Включіть зображення з різних кутів та відстаней\n",
    "  - Зафіксуйте варіації освітлення\n",
    "  - Включіть різні фони та можливі перешкоди\n",
    "  - Мінімум 50 зображень на клас для простого доказу концепції\n",
    "\n",
    "### Підготовка даних:\n",
    "\n",
    "1. **Маркування зображень**:\n",
    "   - Окреслення обмежувальних рамок навколо об'єктів\n",
    "   - Присвоєння класу (мітки) кожній рамці\n",
    "   - Перегляд та коригування міток за потреби\n",
    "\n",
    "2. **Обробка даних**:\n",
    "   - **Розділення набору даних**: Автоматичний поділ на тренувальну, валідаційну та тестову вибірки\n",
    "   - **Аугментація**: Створення додаткових тренувальних зразків через обертання, віддзеркалення, регулювання яскравості тощо\n",
    "   - **Попередня обробка**: Зміна розміру зображень, коригування кольорів тощо"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12ed93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Завантаження та підготовка наборів даних\n",
    "\n",
    "Після маркування даних у Roboflow, необхідно завантажити їх для локального навчання моделі. Розглянемо як це зробити та підготувати дані для тренування.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c051a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Структура набору даних для навчання\n",
    "\n",
    "1. **Структура каталогів YOLO**:\n",
    "   - `/data/назва_проекту/` - основний каталог\n",
    "   - `/train/` - зображення та мітки для навчання\n",
    "   - `/valid/` - зображення та мітки для валідації\n",
    "   - `/test/` - зображення та мітки для тестування\n",
    "\n",
    "2. **Формат міток**:\n",
    "   - Текстові файли `.txt` для кожного зображення\n",
    "   - Кожен рядок містить: `клас x_центр y_центр ширина висота`\n",
    "   - Координати нормалізовані від 0 до 1\n",
    "\n",
    "3. **Організація даних**:\n",
    "   - Зображення: JPG або PNG формати\n",
    "   - Однакове розміщення файлів зображень та міток\n",
    "   - Належне розділення між навчальним, валідаційним та тестовим наборами\n",
    "\n",
    "![Пояснювання маркування зображень](assets/two-persons-tie.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc505bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Налаштування конфігураційного файлу\n",
    "\n",
    "1. **Файл data.yaml**:\n",
    "   ```yaml\n",
    "   path: ../data/назва_проекту  # Шлях до набору даних\n",
    "   train: train/images          # Зображення для тренування\n",
    "   val: valid/images            # Зображення для валідації\n",
    "   test: test/images            # Зображення для тестування\n",
    "   \n",
    "   nc: 2                        # Кількість класів\n",
    "   names: ['об'єкт1', 'об'єкт2'] # Назви класів"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ea8ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Завантаження через Roboflow API\n",
    "\n",
    "Після маркування даних у Roboflow, необхідно завантажити їх для локального навчання моделі. Розглянемо як це зробити та підготувати дані для тренування."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a306014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Встановимо або перевіримо наявність бібліотек, які потрібні для роботи\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26432b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпортуємо необхідні бібліотеки\n",
    "from roboflow import Roboflow\n",
    "import torch\n",
    "\n",
    "# Перевірка доступності CUDA (не застосовується для Apple Silicon, але корисно включити для портативності)\n",
    "print(f\"Версія PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Використовується CUDA\")\n",
    "# Перевірка доступності MPS (Metal Performance Shaders) для Apple Silicon\n",
    "elif hasattr(torch.backends, 'mps') and hasattr(torch.backends.mps, 'is_available'):\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Використовується MPS (Metal Performance Shaders)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS не доступний у цій версії PyTorch, використовується CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aaaa02",
   "metadata": {},
   "source": [
    "### Завантаження даних з Roboflow\n",
    "\n",
    "Щоб завантажити дані з Roboflow, вам знадобиться API-ключ та інформація про проект:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завантаження даних з Roboflow\n",
    "# Зверніть увагу, що для використання Roboflow API вам потрібно зареєструватися на їхньому сайті та отримати API ключ\n",
    "# Зазначте, що ви повинні мати файл .env з вашим API ключем\n",
    "# або ви можете вказати його безпосередньо в коді (не рекомендується з міркувань безпеки)\n",
    "\n",
    "# Також, необхідно вказати назву вашого проекту та версію набору даних\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Завантаження змінних середовища з файлу .env\n",
    "# Переконайтеся, що у вас є файл .env з вашим API ключем\n",
    "load_dotenv()\n",
    "\n",
    "# Отримання API ключа з змінних середовища\n",
    "api_key = os.getenv('ROBOFLOW_API_KEY')\n",
    "\n",
    "# Визначення версії набору даних\n",
    "dataset_version = 3  # Змініть на вашу фактичну версію набору даних\n",
    "\n",
    "rf = Roboflow(api_key=api_key)\n",
    "project = rf.workspace().project(\"orange-ball-detection\")\n",
    "version = project.version(dataset_version)\n",
    "\n",
    "# Завантаження набору даних у форматі YOLO (сумісний з YOLOv8)\n",
    "dataset = version.download(model_format=\"yolov8\", location=f\"data/robotics_dataset_v{dataset_version}\", overwrite=False)\n",
    "\n",
    "print(\"Завантаження завершено.\")\n",
    "print(\"Набір даних успішно завантажено до:\", dataset.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc8500",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Локальне навчання моделі комп'ютерного зору\n",
    "\n",
    "Тепер, коли ми підготували наші дані, настав час навчити модель комп'ютерного зору. Ми будемо використовувати базову модель YOLOv8n (YOLOv8 Nano) для виявлення об'єктів. Це легка модель, яка підходить для навчання на комп'ютерах з обмеженими ресурсами, таких як MacBook з M1.\n",
    "\n",
    "### Варіанти навчання\n",
    "\n",
    "1. **Локальне навчання на Mac M1**:\n",
    "   - Підходить для невеликих наборів даних та швидких демонстрацій\n",
    "   - Використовує Metal Performance Shaders (MPS) для прискорення\n",
    "   - Обмежено доступною пам'яттю та обчислювальною потужністю\n",
    "2. **Навчання на (віддаленому) високопродуктивному GPU (рекомендується)**:\n",
    "   - Значно швидше навчання з GPU, такими як NVIDIA 3090Ti\n",
    "   - Може обробляти більші моделі (середні, великі) та більше даних\n",
    "   - Створіть таку ж структуру каталогів на віддаленій машині\n",
    "   - Скопіюйте свій набір даних на віддалену машину\n",
    "   - Запустіть код навчання там і завантажте файл ваги після завершення"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b66cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпортуємо необхідні бібліотеки\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Налаштування шляху до файлу data.yaml\n",
    "# Зазначте, що YOLOv8 автоматично генерує data.yaml файл під час завантаження набору даних\n",
    "data_yaml = f\"{dataset.location}/data.yaml\"\n",
    "\n",
    "# Для локального навчання на Mac M1 (для демонстраційних цілей)\n",
    "# Ініціалізуємо попередньо навчену модель YOLOv8n (маленький розмір для швидшого навчання)\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Тренування моделі\n",
    "# Зверніть увагу, що для реального навчання вам слід використовувати більше епох\n",
    "# та налаштувати параметри навчання відповідно до ваших потреб\n",
    "# Для демонстраційних цілей ми зменшили кількість епох до 10\n",
    "results = model.train(\n",
    "    data=data_yaml,     # Шлях до файлу data.yaml\n",
    "    epochs=50,          # Кількість епох\n",
    "    patience=5,         # Кількість епох без покращення для ранньої зупинки\n",
    "    imgsz=640,          # Розмір зображення (640px)\n",
    "    save_period=1,      # Зберігати модель кожну епоху\n",
    "    device=device,      # Пристрій для навчання (визначили вище)\n",
    "    batch=4,            # Розмір пакету (налаштуйте на основі RAM)\n",
    "    workers=2,          # Кількість потоків для завантаження даних\n",
    "    cache='disk',       # Кешувати зображення для швидшого навчання\n",
    "    project=\"models\",   # Назва проекту для збереження результатів\n",
    "    name=f\"robotics_model_v{dataset_version}\",  # Назва моделі \n",
    "    amp=True,            # Увімкнути змішану точність для швидкості та економії пам'яті\n",
    ")\n",
    "\n",
    "print(f\"Навчання завершено! Модель збережено в ./models/robotics_model_v{dataset_version}/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c7a76",
   "metadata": {},
   "source": [
    "### Поради та оптимізація навчання\n",
    "\n",
    "#### Для Apple Silicon\n",
    "\n",
    "1. **Використовуйте прискорення MPS**: PyTorch 2.0+ підтримує Metal Performance Shaders (MPS) для Mac M1/M2.\n",
    "\n",
    "2. **Оптимізація розміру пакету**: Почніть з розміру пакету 16 та коригуйте відповідно до доступної пам'яті.\n",
    "\n",
    "3. **Вибір розміру моделі**: Використовуйте менші моделі, такі як YOLOv8n або YOLOv8s, для швидшого локального навчання.\n",
    "\n",
    "4. **Кешування даних**: Використовуйте параметр `cache=True` для прискорення навчання.\n",
    "\n",
    "#### Для NVIDIA 3090Ti або інших GPU\n",
    "\n",
    "1. **Більший розмір пакету**: Використовуйте розміри пакетів 32 або 64, щоб повністю використовувати пам'ять GPU.\n",
    "\n",
    "2. **Кілька робочих потоків**: Збільште значення `workers` до 8 або 16 для швидшого завантаження даних.\n",
    "\n",
    "3. **Більші моделі**: Використовуйте переваги потужного GPU, застосовуючи моделі YOLOv8m або YOLOv8l.\n",
    "\n",
    "4. **Змішана точність**: Додайте `amp=True` для використання навчання зі змішаною точністю для кращої продуктивності.\n",
    "\n",
    "5. **Більше епох**: Навчайте модель протягом 50-100 епох для кращої конвергенції.\n",
    "\n",
    "#### Загальні поради\n",
    "\n",
    "1. **Трансферне навчання**: Ми використовуємо попередньо навчені моделі та дооптимізуємо їх, що значно пришвидшує навчання.\n",
    "\n",
    "2. **Аугментація зображень**: Використовуйте можливості аугментації Roboflow для збільшення різноманітності набору даних."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02015e",
   "metadata": {},
   "source": [
    "## Deploying the Model for Inference with a USB Camera\n",
    "## Запуск моделі з USB-камерою\n",
    "\n",
    "З натренованою моделлю ми можемо використовувати її для виявлення об'єктів у реальному часі за допомогою USB-камери."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df098668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Імпортуємо необхідні бібліотеки\n",
    "# Примітка: цей код призначений для запуску в окремому Python скрипті, а не в Jupyter\n",
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import argparse\n",
    "\n",
    "# Зберемо шлях до моделі\n",
    "model_path = f\"./models/robotics_model_v{dataset_version}/weights/best.pt\"\n",
    "\n",
    "# Функція для запуску об'єктного виявлення на USB-камері\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Запуск об\\'єктного виявлення на USB-камері')\n",
    "    parser.add_argument('--model', type=str, default='yolov8n.pt', \n",
    "                        help='Шлях до моделі YOLOv8')\n",
    "    parser.add_argument('--conf', type=float, default=0.25, \n",
    "                        help='Поріг впевненості для виявлення об\\'єктів')\n",
    "    parser.add_argument('--camera', type=int, default=0, \n",
    "                        help='Індекс камери (зазвичай 0 для вбудованої, 1 для USB)')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Завантаження моделі\n",
    "    try:\n",
    "        model = YOLO(args.model)\n",
    "        print(f\"Завантажено модель з {args.model}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Помилка при завантаженні моделі: {e}\")\n",
    "        return\n",
    "\n",
    "    # Відкриття камери\n",
    "    cap = cv2.VideoCapture(args.camera)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Помилка: Не вдалося відкрити камеру з індексом {args.camera}\")\n",
    "        print(\"Спробуйте інший індекс камери, використовуючи аргумент --camera\")\n",
    "        return\n",
    "\n",
    "    # Налаштування параметрів камери\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    print(\"Камера успішно відкрита. Натисніть 'q', щоб вийти.\")\n",
    "\n",
    "    # Змінні для обчислення FPS\n",
    "    fps = 0\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        # Зчитування кадру з камери\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Помилка: Не вдалося зчитати зображення з камери\")\n",
    "            break\n",
    "\n",
    "        # Збільшення лічильника кадрів  \n",
    "        frame_count += 1\n",
    "\n",
    "        # Виконання інференсу\n",
    "        results = model(frame, conf=args.conf)\n",
    "\n",
    "        # Обробка результатів\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Обчислення FPS\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time >= 1.0:\n",
    "            fps = frame_count / elapsed_time\n",
    "            frame_count = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "        cv2.putText(annotated_frame, f\"FPS: {fps:.2f}\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Відображення кадру\n",
    "        cv2.imshow('Об\\'єктне виявлення', annotated_frame)\n",
    "        \n",
    "        # Вихід з циклу при натисканні 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Завершення роботи\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a906db",
   "metadata": {},
   "source": [
    "## Запуск скрипта для виведення\n",
    "\n",
    "Щоб запустити скрипт виведення з вашою USB-камерою:\n",
    "\n",
    "1. Переконайтеся, що ваше віртуальне середовище активоване:\n",
    "   ```\n",
    "   source venv/bin/activate  # На macOS/Linux\n",
    "   ```\n",
    "   або\n",
    "   ```\n",
    "   venv\\Scripts\\activate  # На Windows\n",
    "   ```\n",
    "2. Запустіть скрипт з терміналу:\n",
    "   ```\n",
    "   python run_robot_vision.py\n",
    "   ```\n",
    "3. Ви можете налаштувати поведінку за допомогою цих параметрів:\n",
    "   - Щоб використовувати певну камеру: `--camera 1` \n",
    "   - Щоб використовувати вашу натреновану модель: `--model ./models/robotics_model/weights/best.pt`\n",
    "   - Щоб налаштувати впевненість виявлення: `--conf 0.4`\n",
    "\n",
    "Для прикладу:\n",
    "\n",
    "```\n",
    "python run_robot_vision.py --camera 1 --model ./models/robotics_model/weights/best.pt --conf 0.4\n",
    "```\n",
    "\n",
    "Якщо у вас виникають проблеми з індексом камери, спробуйте різні значення (0, 1, 2 тощо), поки не знайдете те, що відповідає вашій USB-камері."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bded456",
   "metadata": {},
   "source": [
    "## Висновок та наступні кроки\n",
    "\n",
    "Вітаємо! Тепер ви успішно:\n",
    "\n",
    "1. ✅ Дізналися, як машинне навчання може покращити завдання промислової робототехніки  \n",
    "2. ✅ Створили й проанотували набір даних за допомогою Roboflow  \n",
    "3. ✅ Підготували дані для локального навчання  \n",
    "4. ✅ Навчили власну модель комп’ютерного зору, оптимізовану для вашого M1 Mac  \n",
    "5. ✅ Розгорнули модель для реального часу з USB-камерою  \n",
    "\n",
    "### Що робити далі\n",
    "\n",
    "Тепер, коли у вас є базові навички, ось кілька шляхів для подальшого розвитку:\n",
    "\n",
    "1. **Створіть більший набір даних**: спробуйте зібрати більше знімків у різних умовах освітлення та під різними кутами, щоб підвищити стійкість моделі.  \n",
    "2. **Експериментуйте з різними моделями**: YOLOv8 доступний у різних розмірах (nano, small, medium, large). Навчіть той самий набір даних на різних архітектурах і порівняйте швидкість та точність. Також спробуйте інші моделі YOLO (наприклад, YOLOv11). \n",
    "3. **Оптимізація швидкодії**: досліджуйте методи квантизації або обрізки (pruning), щоб модель працювала ще швидше на низько ресурсних пристроях які будуть встановлені безпосередньо біля роботів.  \n",
    "4. **Налаштування мультикамерної системи**: для більш складних застосунків встановіть кілька камер для отримання різних ракурсів тієї самої сцени.  \n",
    "\n",
    "### Корисні ресурси\n",
    "\n",
    "- [Документація Ultralytics YOLOv8](https://docs.ultralytics.com/) — детальні поради щодо роботи з моделлю  \n",
    "- [Документація Roboflow](https://docs.roboflow.com/) — розширені прийоми аугментації та розмітки даних  \n",
    "- [Документація PyTorch](https://pytorch.org/docs/) — щоб зрозуміти фреймворк глибинного навчання  \n",
    "- [OpenCV Python Tutorials](https://docs.opencv.org/master/d6/d00/tutorial_py_root.html) — для просунутих операцій комп’ютерного зору  \n",
    "\n",
    "Не забувайте тримати віртуальне середовище активним під час роботи над проєктом та не соромтеся експериментувати з моделлю на різних об’єктах!  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "rise": {
   "height": "100%",
   "scroll": false,
   "transition": "fade",
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
